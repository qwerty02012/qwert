from playsound import playsound
from gtts import gTTS
text=""
language="en"
texttosound=gTTS(text=text,lang=language)
texttosound.save("output.mp3")
playsound("output.mp3")
print("Sound Played")


import speech_recognition as sr
file="path"
r=sr.Recognizer()
with sr.AudioFile(file)as source:
    speechtotext= r.record(source)
    text=r.recognize_google(speechtotext)
    print(text)


#corpus


import nltk
from nltk.corpus import brown

print('File ids of brown corpus\n',brown.fileids())
ca01 = brown.words('ca01')
#display first few words
print('\nca01 has following words:\n',ca01)
#total number of words in ca01
print('\nca01 has',len(ca01),'words')
#categories or files
print('\nCategories or files in brown corpus:\n')
print(brown.categories())
print('nStats for each text\n')
print('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\t\tFileName')
for fileid in brown.fileids():
    num_chars=len(brown.raw(fileid))
    num_words=len(brown.words(fileid))
    num_sents=len(brown.sents(fileid))
    num_vocab=len(set([w.lower() for w in brown.words(fileid)]))
    print(int(num_chars/num_words),'\t\t',int(num_words/num_sents),'\t\t',int(num_words/num_vocab),'\t\t',fileid)




#own_corpus

import nltk
from nltk.corpus import PlaintextCorpusReader
corpusdir = "path"

filelist = PlaintextCorpusReader(corpusdir,'.*')
print('\n File List :\n')
print(filelist.fileids())
print(filelist.root)
print('AvgWordLen\tAvgSentLen\tno.ofTimesEachWordAppearsonAvg\tFileName')
for fileid in filelist.fileids() :
 num_char=len(filelist.raw(fileid))
 num_words=len(filelist.words(fileid))
 num_sents=len(filelist.sents(fileid))
 num_vocab=len([w.lower() for w in filelist.words(fileid)])
 print(int(num_char/num_words),'\t\t',
 int(num_words/num_sents),'\t\t',
 int(num_words/num_vocab),'\t\t',fileid)



#tokenization

import nltk
from nltk import tokenize
para=""
sents=tokenize.sent_tokenize(para)
print("Sentence Tokenization\n-------------\n",sents)
print('\nWord Tokenization\n-----------\n')
for i in range(len(sents)):
 words = tokenize.word_tokenize(sents[i])
 print(words)



#most_freqnountag

import nltk
from collections import defaultdict
text = nltk.word_tokenize("Nick likes to play football. Nick does not like to play Cricket.")
tagged = nltk.pos_tag(text)
print(tagged)
# checking if it is a noun or not 
addNounWords = []
count = 0
for words in tagged:
    val = tagged[count][1]
    if(val=='NN' or val=='NNS' or val=='NNP' or val=='NNPs'):
        addNounWords.append(tagged[count][0])
    count+=1
print(addNounWords)
temp = defaultdict(int)
# memoizing count
for sub in addNounWords:
    for wrd in sub.split():
        temp[wrd]+=1
# getting max frequency
res = max(temp,key=temp.get)
# printing result
print('Word with max frequency :'+str(res))


#map using dict

thisdict={"brand":"Ford","model":"Mustang","year":"1994"}
print(thisdict)
print(thisdict["brand"])
print(len(thisdict))
print(type(thisdict))

default tag, reg exp
import nltk
from nltk.tag import DefaultTagger
deftagger = DefaultTagger('NN')
print(deftagger.tag_sents("This is NLP lecture in room number 305. Today we are going to learn tokenization."))

from nltk.corpus import brown
from nltk.tag import RegexpTagger
test_sent = brown.sents(categories='news')[0]
regexp_tagger = RegexpTagger(
[(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers
(r'(The|the|A|a|An|an)$', 'AT'), # articles
(r'.*able$', 'JJ'), # adjectives
(r'.*ness$', 'NN'), # nouns formed from adjectives
(r'.*ly$', 'RB'), # adverbs
(r'.*s$', 'NNS'), # plural nouns
(r'.*ing$', 'VBG'), # gerunds
(r'.*ed$', 'VBD'), # past tense verbs
(r'.*', 'NN') # nouns (default)
])
print(regexp_tagger)
print(regexp_tagger.tag(test_sent))
