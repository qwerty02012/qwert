#lemmas_hypername_hypo
import nltk
from nltk.corpus import wordnet
print(wordnet.synsets("computer"))
print(wordnet.synset("computer.n.01").lemma_names())

print(wordnet.synset('computer.n.01').lemmas())

print(wordnet.lemma('computer.n.01.computing_device').synset())

print(wordnet.lemma('computer.n.01.computing_device').name())

#Hyponyms give abstract concepts of the word that are much more specific
#the list of hyponyms words of the computer
syn = wordnet.synset('computer.n.01')
print(syn.hyponyms)
print([lemma.name() for synset in syn.hyponyms() for lemma in synset.lemmas()])
#the semantic similarity in WordNet
vehicle = wordnet.synset('vehicle.n.01')
car = wordnet.synset('car.n.01')
print(car.lowest_common_hypernyms(vehicle))




#synset_def_an
import nltk
from nltk.corpus import wordnet
print(wordnet.synsets("all"))
print(wordnet.synset("colour.n.01").examples())
print(wordnet.synset("animal.n.01").definition())
print(wordnet.lemma('buy.v.01.buy').antonyms())



#genism_adding_removing
import gensim
from nltk.tokenize import word_tokenize
from gensim.parsing.preprocessing import remove_stopwords,STOPWORDS
text = "Sahil likes to play Cricket, however he is not too fond of tennis."
all_stopwords_gensim =STOPWORDS.union(set(['like', 'play']))
all_stopwords = gensim.parsing.preprocessing.STOPWORDS
text_tokens =word_tokenize(text)
tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]
print(tokens_without_sw)
sw_list = {"not"}
text_tokens =word_tokenize(text)
tokens_without_sw = [word for word in text_tokens if not word in sw_list]
print(tokens_without_sw)



#spacy_adding_removing
import spacy
import nltk
from nltk.tokenize import word_tokenize
sp = spacy.load('en_core_web_sm')
stopwords = sp.Defaults.stop_words
stopwords.add('play')
text = "Sahil likes to play Cricket, however he is not too fond of tennis."
text_tokens = word_tokenize(text)
tokens_without_sw = [word for word in text_tokens if not word in stopwords]
print(tokens_without_sw)
stopwords.remove('not')
tokens_without_sw = [word for word in text_tokens if not word in stopwords]
print(tokens_without_sw)


#split
text.split(' ')


#regular exptokenizer
from nltk.tokenize import RegexpTokenizer
tk = RegexpTokenizer('\s+', gaps=True)
text = "Exploring the world through travel has both positive and negative implications."
print(tk.tokenize(text))




#spacy
import spacy
nlp = spacy.blank('en')
text = "Exploring the world through travel has both positive and negative implications."
doc = nlp(text)
words = [word.text for word in doc]
print(words)



#keras

from keras.preprocessing.text import text_to_word_sequence
text = "Exploring the world through travel has both positive and negative implications."
sent = text_to_word_sequence(text)
print(text_to_word_sequence(text))


#gensim

from gensim.utils import tokenize
text = "Exploring the world through travel has both positive and negative implications."
print(list(tokenize(text)))
